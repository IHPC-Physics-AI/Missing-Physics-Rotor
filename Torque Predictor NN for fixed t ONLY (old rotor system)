import jax
import jax.numpy as jnp
from jax import jit
from flax import linen as nn
from flax.training import train_state
import optax
from functools import partial
from jax import config
config.update("jax_enable_x64", True) #Critical for tiny values
import matplotlib.pyplot as plt
# Initialize random key
rng = jax.random.PRNGKey(42)

# Constants
I = 4.667e-10  # Moment of inertia (kg·m²)
gamma = 0.118  # Friction coefficient (rad/s)
c = I * gamma  # Damping coefficient (N·m·s)
epsilon = 8.854e-12  # Permittivity (F/m)
A = 2e-6  # Effective overlap area between electrodes and rotor arms (m²)
V0 = 600  # Peak voltage (V)
R = 0.005  # Radius of rotor (m)
d0 = 0.001  # Baseline distance (m)

# Rotor arms and electrode angles
arm_angles = jnp.array([0, jnp.pi / 2, jnp.pi, (3*jnp.pi)/2])  # Arms 1 to 4. Creates 1D array.
electrode_angles = jnp.linspace(0, 2 * jnp.pi, 6, endpoint=False) # 6 electrodes

# Precomputed constants
R2 = 2 * R
d0_squared = d0**2
epsilon_A = epsilon * A
R_epsilon_A = epsilon_A * R

# Precompute triangular waveforms on a time grid
time_grid = jnp.linspace(0, 1, 1000)
V1_table = V0 * (2 * jnp.abs(2 * (time_grid % 1) - 1) - 1)
V2_table = jnp.roll(V1_table, jnp.floor(len(time_grid) / 3).astype(int)) #JAX has explicit emphasis on data types.
V3_table = jnp.roll(V1_table, jnp.floor(2 * len(time_grid) / 3).astype(int))

@jit #will be slower on first run, but thereafter runs will be faster.
def electrode_voltages_precomputed(t, f):
    # Use jnp.floor to calculate the index as a JAX array and convert to int
    idx = jnp.floor((f * t) % 1 * len(time_grid)).astype(int) #.astype designed for arrays use
    v1 = V1_table[idx]
    v2 = V2_table[idx]
    v3 = V3_table[idx]
    return jnp.array([v1, v2, v3, v1, v2, v3])

# Torque computation #Torque is a function of position
def compute_torques_scalar(theta, t, f):
    angle_diff = theta + arm_angles[:, jnp.newaxis] - electrode_angles[jnp.newaxis, :]
    sin_half_angle_diff = jnp.sin(angle_diff / 2)
    distances_squared = d0_squared + (R2 * sin_half_angle_diff)**2
    voltages_squared = electrode_voltages_precomputed(t, f)**2
    torques = R_epsilon_A * voltages_squared / distances_squared * jnp.sign(jnp.sin(angle_diff))
    return jnp.sum(torques)

# Define a function that takes theta and f and uses the fixed t
def compute_torques(theta, f):
    return compute_torques_scalar(theta, t=20.0, f=f)
# =============================================
# Enhanced Scaling (Handles Tiny Values)
# =============================================
def scale_torque(torque):
    """Scale torque to [-1, 1] range before normalization"""
    scale_factor = 1e8  # Adjust based on your torque magnitude
    scaled = torque * scale_factor
    return scaled, scale_factor
    #log_torque = jnp.log10(jnp.abs(torque) + 1e-20)  # Adjust based on your torque magnitude
    #scaled = log_torque / jnp.max(jnp.abs(jnp.log10(jnp.abs(torque) + 1e-20))) #Normalize to [-1, 1]
    #return scaled, None

# Generate the training data for theta
theta_train = jnp.linspace(0, 2 * jnp.pi, 10000)

# Generate the training data for frequency f_train
f_train = jnp.linspace(0.5, 8.5, 10000)

# Define min-max scaling functions
def min_max_scale(data, min_val, max_val):
    return (data - min_val) / (max_val - min_val)

# Scale the training data
theta_min, theta_max = jnp.min(theta_train), jnp.max(theta_train)
f_min, f_max = jnp.min(f_train), jnp.max(f_train)

theta_train_scaled = min_max_scale(theta_train, theta_min, theta_max).reshape(-1, 1)
f_train_scaled = min_max_scale(f_train, f_min, f_max).reshape(-1, 1)

# Use vmap to apply compute_torques over the inputs
torque_train = jax.vmap(compute_torques, in_axes=(0, 0))(theta_train, f_train)

# Scale torque values
# torque_train_scaled, torque_scaled = scale_torque(torque_train)

def scale_torque(torque):
    mean = jnp.mean(torque)
    std = jnp.std(torque)
    scaled = (torque - mean) / std
    return scaled, (mean, std)

def unscale_torque(torque_scaled, scale_params):
    mean, std = scale_params
    return torque_scaled * std + mean

torque_train_scaled, scale_params = scale_torque(torque_train)

# Reshape data
theta_train_scaled = theta_train_scaled.reshape(-1, 1)
torque_train_scaled = torque_train_scaled.reshape(-1, 1)
f_train_scaled = f_train_scaled.reshape(-1, 1)  # Reshape f_train
class TorquePredictor(nn.Module):
    dropout_rate: float = 0.1  # Set dropout rate

    @nn.compact
    def __call__(self, theta, f, deterministic: bool):
        # Concatenate theta with frequency f
        x = jnp.concatenate([theta, f], axis=-1)

        # Forward pass through layers with improved activation functions
        x = nn.Dense(512)(x)  # Increased layer size
        x = jnp.sin(2*jnp.pi*x) + jnp.cos(2*jnp.pi*x) # Try Leaky ReLU. discretised relu can represent curves. relu is powerful but finite networks can be better used with more fitting functions.

        x = nn.Dense(512)(x)
        x = nn.swish(x)

        # Apply dropout
        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)

        x = nn.Dense(256)(x)  # Another layer
        x = nn.leaky_relu(x) #relu is the most generalizable. does a piecewise breakdown of curves. relu cannot get 2nd order derivatives.

        x = nn.Dense(256)(x)
        x = nn.swish(x) #in general try swish, sine and cosine, tanh. No hard rule. swish is exponential pattern. similar to sigmoid. in theory, infinitely wide network can be represented by sine and cosine.

        # Apply dropout
        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=deterministic)

        x = nn.Dense(128)(x)  # And another layer
        x = nn.relu(x)

        x = nn.Dense(64)(x)
        x = nn.tanh(x)
        return nn.Dense(1)(x)
# Initialize model
model = TorquePredictor()
params = model.init(rng, jnp.ones((1, 1)), jnp.ones((1, 1)), deterministic = True)

# =============================================
# Robust Training Setup
# =============================================
optimizer = optax.chain( #Combines multiple transformations to apply to the optimisation process in order into a single transformation.
    optax.clip(1.0),  # Gradient clipping to prevent them from exceeding a certain magnitude. Avoid issues of numerical instability caused by exploding gradients.
    optax.adam(learning_rate=1e-3)
)
state = train_state.TrainState.create(
    apply_fn=model.apply,
    params=params,
    tx=optimizer
)
# Training step function
@jax.jit
def train_step(state, batch, dropout_rng):
    theta_batch, torque_batch, f_batch = batch
    def loss_fn(params):
        pred = state.apply_fn(params, theta_batch, f_batch, deterministic=False, rngs={'dropout': dropout_rng})
        return jnp.mean((pred - torque_batch) ** 2)

    loss, grads = jax.value_and_grad(loss_fn)(state.params)
    return state.apply_gradients(grads=grads), loss

# Training loop
for epoch in range(5000):
    rng, dropout_rng, next_rng = jax.random.split(rng, 3)
    state, loss = train_step(state, (theta_train_scaled, torque_train_scaled, f_train_scaled), dropout_rng)
    rng = next_rng
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")
# =============================================
# Evaluation
# =============================================

# Generate theta_test values
theta_test = jnp.linspace(0, 2 * jnp.pi, 100).reshape(-1, 1)
f_test = jnp.linspace(0.5, 8.5, 100).reshape(-1, 1)
theta_test_scaled = min_max_scale(theta_test, theta_min, theta_max).reshape(-1, 1)
f_test_scaled = min_max_scale(f_test, f_min, f_max).reshape(-1, 1)

# Compute true torque with a specific frequency for comparison
torque_true = jax.vmap((compute_torques))(theta_test.squeeze(), f_test.squeeze())
#torque_true = jnp.array([compute_torques_scalar(theta, t=20.0, f=f) for theta, f in zip(theta_test.squeeze(), f_test.squeeze())])

# Predict and rescale
torque_pred_scaled = model.apply(state.params, theta_test_scaled, f_test_scaled, deterministic = True)
torque_pred = unscale_torque(torque_pred_scaled)  # Reverse scaling
# Create the plot
fig, ax1 = plt.subplots(figsize=(12, 6))

# Plot true torque and predicted torque on the primary y-axis
ax1.plot(theta_test, torque_true, label='True Torque', linewidth=2, color='blue')
ax1.plot(theta_test, torque_pred, '--', label='NN Prediction', linewidth=2, color='orange')
ax1.set_xlabel('Theta (radians)')
ax1.set_ylabel('Torque (Original Scale)', color='black')
ax1.tick_params(axis='y', labelcolor='black')
ax1.legend(loc='upper left')
ax1.grid(True)

# Create a secondary x-axis for frequency
ax2 = ax1.twiny()  # Instantiate a second axes that shares the same y-axis

# Optional: Set limits for clarity if needed
# ax2.set_xlim(0.5, 8.5)  # Adjust x-limits for frequency if needed
# Add legend for frequency
ax2.legend(loc='upper right')

# Show the title and the plot
plt.title('Improved Torque Prediction with Frequency')
plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()

# Metrics
mse = jnp.mean((torque_true - torque_pred)**2)
print(f"Test MSE: {mse:.3e}")
print(f"Max Error: {jnp.max(jnp.abs(torque_true - torque_pred)):.3e}")
